# Memory Management Best Practices
## Qu·∫£n l√Ω b·ªô nh·ªõ CPU v√† GPU - Tr√°nh Memory Leak

T√†i li·ªáu n√†y t·ªïng h·ª£p c√°c best practices t·ª´ c√°c GitHub repositories v√† c·ªông ƒë·ªìng ƒë·ªÉ qu·∫£n l√Ω memory hi·ªáu qu·∫£.

---

## üìö T√†i li·ªáu tham kh·∫£o

### 1. PyTorch Memory Management
- **PyTorch Forum**: [GPU memory leak discussions](https://discuss.pytorch.org/t/gpu-memory-leak-with-zombie-memory-occupation-after-job-is-killed/220338)
- **Stack Overflow**: [Fix GPU mem leak after minibatches](https://stackoverflow.com/questions/61912734/pytorch-fix-gpu-mem-leak-after-exactly-10-minibatches)
- **Clay Atlas**: [Release GPU/CPU memory in PyTorch](https://clay-atlas.com/us/blog/2024/01/09/pytorch-release-gpu-cpu-memory/)

### 2. YOLO/Ultralytics
- **Ultralytics Docs**: Memory management trong YOLO inference
- **GitHub Issues**: C√°c v·∫•n ƒë·ªÅ v·ªÅ memory leak trong YOLO

### 3. OpenCV & NumPy
- **Stack Overflow**: OpenCV memory leak prevention
- **NumPy Docs**: Array memory management

### 4. Python Threading
- **Python Docs**: Memory management trong multi-threading
- **Gist**: [Memory leak prevention in threading](https://gist.github.com/odhondt/014e39acc31cca945d636e4b4d74e1a5)

---

## üîß Best Practices

### 1. GPU Memory Management (PyTorch/CUDA)

#### a) Release GPU Tensors
```python
# ‚ùå SAI: Gi·ªØ tensor tr√™n GPU
tensor = model(input).cuda()

# ‚úÖ ƒê√öNG: Move v·ªÅ CPU v√† release GPU reference
tensor_gpu = model(input).cuda()
tensor_cpu = tensor_gpu.cpu().detach()
del tensor_gpu  # Release GPU memory
```

#### b) Clear GPU Cache
```python
import torch
import gc

# Sau khi x·ª≠ l√Ω xong m·ªôt batch
torch.cuda.empty_cache()  # Clear unused GPU memory
gc.collect()  # Force Python garbage collection
```

#### c) Use Context Managers
```python
# T·ª± ƒë·ªông release khi out of scope
with torch.no_grad():
    results = model.predict(frame)
    # Process results
    # GPU tensors s·∫Ω ƒë∆∞·ª£c release t·ª± ƒë·ªông
```

#### d) Detach v√† Move to CPU
```python
# Khi kh√¥ng c·∫ßn gradient
tensor = tensor.detach().cpu().numpy()
# Ho·∫∑c
tensor = tensor.cpu().detach().numpy()
```

---

### 2. CPU Memory Management (NumPy/OpenCV)

#### a) Release NumPy Arrays
```python
# ‚ùå SAI: Gi·ªØ nhi·ªÅu copies
array1 = frame.copy()
array2 = frame.copy()
array3 = frame.copy()

# ‚úÖ ƒê√öNG: Release sau khi d√πng
array1 = frame.copy()
# ... use array1 ...
del array1  # Ho·∫∑c ƒë·ªÉ GC t·ª± ƒë·ªông

# Ho·∫∑c reuse n·∫øu c√≥ th·ªÉ
working_array = frame.copy()
# ... process ...
working_array = None  # Allow GC
```

#### b) OpenCV Memory Management
```python
# OpenCV t·ª± ƒë·ªông qu·∫£n l√Ω memory cho Mat objects
# Nh∆∞ng c·∫ßn ch√∫ √Ω:
# - Copy ch·ªâ khi c·∫ßn thi·∫øt
# - Release VideoCapture/VideoWriter khi done

cap = cv2.VideoCapture(url)
# ... use cap ...
cap.release()  # Quan tr·ªçng!
```

#### c) Large Array Operations
```python
# S·ª≠ d·ª•ng in-place operations khi c√≥ th·ªÉ
array += 1  # ‚úÖ In-place
array = array + 1  # ‚ùå T·∫°o copy m·ªõi

# Ho·∫∑c
np.add(array, 1, out=array)  # ‚úÖ In-place
```

---

### 3. Multi-threading Memory Management

#### a) Thread-local Storage
```python
import threading

class Worker:
    def __init__(self):
        self._local = threading.local()
    
    def process(self):
        # Each thread c√≥ storage ri√™ng
        if not hasattr(self._local, 'buffer'):
            self._local.buffer = np.zeros((640, 640, 3))
        # Use self._local.buffer
```

#### b) Atomic Operations
```python
import threading

class SafeCounter:
    def __init__(self):
        self._lock = threading.Lock()
        self._value = 0
    
    def increment(self):
        with self._lock:
            self._value += 1
    
    def get(self):
        with self._lock:
            return self._value
```

#### c) Queue Management
```python
import queue

# Gi·ªõi h·∫°n queue size ƒë·ªÉ tr√°nh memory buildup
frame_queue = queue.Queue(maxsize=2)  # ‚úÖ Good

# Skip old frames
while not queue.empty():
    try:
        old_frame = queue.get_nowait()
        frame = old_frame  # Keep latest
        # Old frames s·∫Ω ƒë∆∞·ª£c GC t·ª± ƒë·ªông
    except queue.Empty:
        break
```

---

### 4. YOLO/Ultralytics Specific

#### a) Release YOLO Results
```python
from ultralytics import YOLO

model = YOLO("model.pt")
results = model.predict(frame, device='cuda')

# ‚ùå SAI: Gi·ªØ to√†n b·ªô results
processed_results = results

# ‚úÖ ƒê√öNG: Extract c·∫ßn thi·∫øt v√† release
boxes = results[0].boxes
masks = results[0].masks

# Process boxes/masks
if masks is not None:
    for i, mask in enumerate(masks.data):
        mask_cpu = mask.cpu().numpy()  # Move to CPU
        # Process mask_cpu
        del mask  # Release GPU tensor

del results  # Release YOLO results
torch.cuda.empty_cache()
```

#### b) Batch Processing
```python
# Process t·ª´ng frame thay v√¨ batch l·ªõn
for frame in frames:
    result = model.predict(frame, device='cuda')
    # Process immediately
    # Result s·∫Ω ƒë∆∞·ª£c GC sau loop iteration
```

---

### 5. Periodic Cleanup

#### a) Scheduled GC
```python
import gc
import time

class PeriodicCleanup:
    def __init__(self, interval=100):
        self.interval = interval
        self.count = 0
    
    def check(self):
        self.count += 1
        if self.count >= self.interval:
            gc.collect()  # Force Python GC
            torch.cuda.empty_cache()  # Clear GPU cache
            self.count = 0

# Usage
cleanup = PeriodicCleanup(interval=100)
for frame in frames:
    process(frame)
    cleanup.check()
```

#### b) Memory Monitoring
```python
import torch
import psutil
import os

def get_memory_usage():
    """Get current memory usage"""
    # CPU memory
    process = psutil.Process(os.getpid())
    cpu_mem = process.memory_info().rss / 1024**2  # MB
    
    # GPU memory
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.memory_allocated() / 1024**2  # MB
    else:
        gpu_mem = 0
    
    return cpu_mem, gpu_mem

# Monitor memory
cpu_mem, gpu_mem = get_memory_usage()
if gpu_mem > 4000:  # 4GB threshold
    torch.cuda.empty_cache()
    gc.collect()
```

---

### 6. Context Managers cho Resources

#### a) Custom Context Manager
```python
from contextlib import contextmanager

@contextmanager
def gpu_memory_manager():
    """Context manager ƒë·ªÉ qu·∫£n l√Ω GPU memory"""
    try:
        yield
    finally:
        import torch
        import gc
        torch.cuda.empty_cache()
        gc.collect()

# Usage
with gpu_memory_manager():
    result = model.predict(frame)
    # Process result
    # Memory s·∫Ω ƒë∆∞·ª£c cleanup t·ª± ƒë·ªông
```

#### b) Resource Cleanup
```python
class ResourceManager:
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Cleanup
        if hasattr(self, 'cap'):
            self.cap.release()
        import gc
        gc.collect()

# Usage
with ResourceManager() as rm:
    rm.cap = cv2.VideoCapture(url)
    # Use rm.cap
    # T·ª± ƒë·ªông release khi done
```

---

## üéØ √Åp d·ª•ng trong Coal Monitoring

### 1. YOLO Inference
```python
# Trong optimized_worker.py
with self.model_lock:
    results = self.model.predict(frame, device=device)
result = results[0] if results else None

# Process result
# ... detection logic ...

# Release GPU tensors sau khi x·ª≠ l√Ω
if result is not None and hasattr(result, 'boxes'):
    # Extract c·∫ßn thi·∫øt
    boxes = result.boxes
    masks = result.masks
    
    # Process v√† release t·ª´ng mask
    for i, mask in enumerate(masks.data):
        mask_cpu = mask.cpu().numpy()
        # Process mask_cpu
        del mask  # Release GPU tensor
    
    # Periodically clear cache
    if self._detection_count % 10 == 0:
        torch.cuda.empty_cache()
```

### 2. Frame Processing
```python
# Skip old frames trong queue
frame = None
while not self._detection_queue.empty():
    try:
        old_frame = self._detection_queue.get_nowait()
        frame = old_frame  # Keep latest
        # Old frames ƒë∆∞·ª£c GC t·ª± ƒë·ªông
    except:
        break

# Process frame
if frame is not None:
    display_frame = frame.copy()  # Only copy when needed
    # ... process ...
    # frame s·∫Ω ƒë∆∞·ª£c GC sau khi out of scope
```

### 3. ROI Masks
```python
# Cache masks nh∆∞ng release intermediate arrays
def _detect_coal_blockage(self, frame, result):
    # Create mask (cached n·∫øu c√≥ th·ªÉ)
    roi_mask = np.zeros((h, w), dtype=np.uint8)
    
    # Process
    for mask_tensor in masks.data:
        mask_cpu = mask_tensor.cpu().numpy()  # Move to CPU
        # Process mask_cpu
        del mask_tensor  # Release GPU reference
    
    # Intermediate arrays s·∫Ω ƒë∆∞·ª£c GC
    return result
```

---

## üìä Memory Monitoring Tools

### 1. GPU Memory Tracking
```python
import torch

def log_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        print(f"GPU: {allocated:.1f} MB allocated, {reserved:.1f} MB reserved")
```

### 2. CPU Memory Tracking
```python
import psutil
import os

def log_cpu_memory():
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024**2
    print(f"CPU: {mem_mb:.1f} MB")
```

### 3. Memory Profiler
```python
# Install: pip install memory-profiler

from memory_profiler import profile

@profile
def process_frame(frame):
    # Code here
    pass
```

---

## ‚ö†Ô∏è Common Mistakes

### 1. ‚ùå Gi·ªØ references kh√¥ng c·∫ßn thi·∫øt
```python
# ‚ùå SAI
self.all_results = []  # Gi·ªØ t·∫•t c·∫£ results
for frame in frames:
    result = model.predict(frame)
    self.all_results.append(result)  # Memory leak!

# ‚úÖ ƒê√öNG
for frame in frames:
    result = model.predict(frame)
    # Process immediately
    process(result)
    # Result ƒë∆∞·ª£c GC t·ª± ƒë·ªông
```

### 2. ‚ùå Kh√¥ng release VideoCapture
```python
# ‚ùå SAI
cap = cv2.VideoCapture(url)
# ... use ...
# Qu√™n release!

# ‚úÖ ƒê√öNG
try:
    cap = cv2.VideoCapture(url)
    # ... use ...
finally:
    cap.release()
```

### 3. ‚ùå Copy kh√¥ng c·∫ßn thi·∫øt
```python
# ‚ùå SAI
frame1 = frame.copy()
frame2 = frame.copy()
frame3 = frame.copy()

# ‚úÖ ƒê√öNG
# Ch·ªâ copy khi c·∫ßn modify m√† kh√¥ng mu·ªën ·∫£nh h∆∞·ªüng original
working_frame = frame.copy() if need_modify else frame
```

---

## üîó Useful Links

1. **PyTorch Memory Management**: https://pytorch.org/docs/stable/notes/cuda.html#memory-management
2. **NumPy Memory**: https://numpy.org/doc/stable/reference/generated/numpy.copy.html
3. **OpenCV Memory**: https://docs.opencv.org/4.x/d3/d63/classcv_1_1Mat.html#details
4. **Python GC**: https://docs.python.org/3/library/gc.html
5. **Threading Best Practices**: https://docs.python.org/3/library/threading.html

---

## üìù Checklist

- [ ] Release GPU tensors sau khi x·ª≠ l√Ω (`.cpu().detach()`)
- [ ] S·ª≠ d·ª•ng `torch.cuda.empty_cache()` ƒë·ªãnh k·ª≥
- [ ] Release VideoCapture/VideoWriter khi done
- [ ] Gi·ªõi h·∫°n queue size ƒë·ªÉ tr√°nh memory buildup
- [ ] Skip old frames thay v√¨ accumulate
- [ ] Cache nh·ªØng g√¨ c√≥ th·ªÉ (ROI masks, polygon arrays)
- [ ] Release intermediate arrays sau khi d√πng
- [ ] S·ª≠ d·ª•ng in-place operations khi c√≥ th·ªÉ
- [ ] Monitor memory usage ƒë·ªãnh k·ª≥
- [ ] S·ª≠ d·ª•ng context managers cho resources

---

**Last Updated**: 2025-01-XX
**Version**: 1.0

