"""
Model Loader Module
===================

Qu·∫£n l√Ω loading v√† caching YOLO models.
H·ªó tr·ª£:
- Load nhi·ªÅu model t·ª´ file (multi-model support)
- Singleton pattern ƒë·ªÉ share models gi·ªØa nhi·ªÅu camera
- Thread-safe inference
- Map camera -> model
"""

import os
import sys
import threading
import torch
import numpy as np
from typing import Optional, Dict, Any, List
from dataclasses import dataclass


@dataclass
class ModelInfo:
    """Th√¥ng tin v·ªÅ model ƒë√£ load"""
    model_id: str
    path: str
    name: str
    class_names: Dict[int, str]
    person_class_id: int
    coal_class_id: int
    cameras: List[int]  # Camera numbers s·ª≠ d·ª•ng model n√†y
    is_loaded: bool = False


class MultiModelLoader:
    """
    Singleton ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu YOLO models
    
    Features:
    - Singleton pattern (share models gi·ªØa cameras)
    - Support multiple models (m·ªói camera d√πng model kh√°c nhau)
    - Thread-safe
    - Lazy loading
    - Auto-detect class IDs
    
    Usage:
        # Load models
        loader = MultiModelLoader.get_instance()
        loader.load_models(system_config)
        
        # Get model for camera
        model_info = loader.get_model_for_camera(camera_number=1)
        
        # Inference
        results = loader.predict(camera_number=1, frame=frame, conf=0.7)
    """
    
    _instance: Optional['MultiModelLoader'] = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """Singleton pattern"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize model loader"""
        if self._initialized:
            return
        
        # Dict[model_id, YOLO model object]
        self._models: Dict[str, Any] = {}
        # Dict[model_id, ModelInfo]
        self._model_infos: Dict[str, ModelInfo] = {}
        # Dict[camera_number, model_id] - mapping camera -> model
        self._camera_model_map: Dict[int, str] = {}
        # Lock cho m·ªói model (thread-safe inference)
        self._inference_locks: Dict[str, threading.Lock] = {}
        
        self._initialized = True
    
    @classmethod
    def get_instance(cls) -> 'MultiModelLoader':
        """L·∫•y singleton instance"""
        return cls()
    
    @property
    def loaded_models(self) -> List[str]:
        """Danh s√°ch model IDs ƒë√£ load"""
        return list(self._models.keys())
    
    def get_model_info(self, model_id: str) -> Optional[ModelInfo]:
        """L·∫•y th√¥ng tin model theo ID"""
        return self._model_infos.get(model_id)
    
    def get_model_info_for_camera(self, camera_number: int) -> Optional[ModelInfo]:
        """L·∫•y th√¥ng tin model cho camera
        
        Args:
            camera_number: S·ªë th·ª© t·ª± camera (1, 2, 3, ...)
            
        Returns:
            ModelInfo ho·∫∑c None
        """
        model_id = self._camera_model_map.get(camera_number)
        if model_id:
            return self._model_infos.get(model_id)
        
        # Fallback: tr·∫£ v·ªÅ model ƒë·∫ßu ti√™n
        if self._model_infos:
            return list(self._model_infos.values())[0]
        return None
    
    def load_from_config(self, system_config) -> Dict[str, bool]:
        """Load t·∫•t c·∫£ models t·ª´ SystemConfig
        
        Args:
            system_config: SystemConfig object
            
        Returns:
            Dict[model_id, success] - k·∫øt qu·∫£ load cho t·ª´ng model
        """
        results = {}
        
        if system_config.models:
            # Load t·ª´ config models
            for model_id, model_cfg in system_config.models.items():
                try:
                    success = self.load(
                        model_id=model_id,
                        model_path=model_cfg.path,
                        model_name=model_cfg.name,
                        cameras=model_cfg.cameras
                    )
                    results[model_id] = success
                except Exception as e:
                    print(f"[ERROR] Load model {model_id} failed: {e}")
                    results[model_id] = False
        else:
            # Backward compatible: load t·ª´ model_path
            try:
                success = self.load(
                    model_id="default",
                    model_path=system_config.model_path,
                    model_name="Default Model",
                    cameras=list(range(1, 10))
                )
                results["default"] = success
            except Exception as e:
                print(f"[ERROR] Load default model failed: {e}")
                results["default"] = False
        
        return results
    
    def load(self, model_id: str, model_path: str, 
             model_name: str = "Model", 
             cameras: List[int] = None) -> bool:
        """Load m·ªôt YOLO model
        
        Args:
            model_id: ID duy nh·∫•t cho model
            model_path: ƒê∆∞·ªùng d·∫´n file model (.pt)
            model_name: T√™n hi·ªÉn th·ªã
            cameras: List camera numbers s·ª≠ d·ª•ng model n√†y
            
        Returns:
            True n·∫øu load th√†nh c√¥ng
            
        Raises:
            FileNotFoundError: N·∫øu kh√¥ng t√¨m th·∫•y file model
        """
        if cameras is None:
            cameras = []
        
        # Ki·ªÉm tra ƒë√£ load ch∆∞a
        if model_id in self._models:
            print(f"[INFO] Model {model_id} ƒë√£ ƒë∆∞·ª£c load, b·ªè qua")
            return True
        
        # T√¨m ƒë∆∞·ªùng d·∫´n model
        resolved_path = self._resolve_model_path(model_path)
        
        if not resolved_path:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file model: {model_path}")
        
        try:
            # Import YOLO (lazy import)
            from ultralytics import YOLO
            
            # T·∫°o lock cho model n√†y
            if model_id not in self._inference_locks:
                self._inference_locks[model_id] = threading.Lock()
            
            with self._inference_locks[model_id]:
                model = YOLO(resolved_path)
                
                # T·ªêI ∆ØU GPU: ƒê∆∞a model l√™n GPU n·∫øu c√≥
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                if device == 'cuda':
                    try:
                        # YOLO t·ª± ƒë·ªông detect GPU, nh∆∞ng c√≥ th·ªÉ explicit set
                        model.to(device)
                        print(f"[GPU] Model {model_id} ƒë√£ ƒë∆∞·ª£c load l√™n GPU")
                        
                        # Warm-up GPU v·ªõi dummy frame (t·ªëi ∆∞u l·∫ßn inference ƒë·∫ßu ti√™n)
                        try:
                            dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)
                            _ = model.predict(
                                dummy_frame,
                                conf=0.5,
                                verbose=False,
                                task='segment',
                                device=device
                            )
                            print(f"[GPU] Model {model_id} ƒë√£ warm-up GPU")
                        except Exception as warmup_error:
                            print(f"[WARN] GPU warm-up failed cho {model_id}: {warmup_error}")
                    except Exception as gpu_error:
                        print(f"[WARN] Kh√¥ng th·ªÉ load model {model_id} l√™n GPU: {gpu_error}")
                        device = 'cpu'
                else:
                    print(f"[CPU] Model {model_id} ch·∫°y tr√™n CPU")
                
                # X√°c ƒë·ªãnh class IDs
                class_names = self._extract_class_names(model)
                person_id = self._find_class_id(class_names, ['person', 'Person'])
                coal_id = self._find_class_id(class_names, ['coal', 'Coal', 'than'])
                
                # L∆∞u model
                self._models[model_id] = model
                self._model_infos[model_id] = ModelInfo(
                    model_id=model_id,
                    path=resolved_path,
                    name=model_name,
                    class_names=class_names,
                    person_class_id=person_id,
                    coal_class_id=coal_id,
                    cameras=cameras,
                    is_loaded=True,
                )
                
                # Map cameras -> model
                for cam_num in cameras:
                    self._camera_model_map[cam_num] = model_id
            
            # Log GPU memory v√† verify GPU n·∫øu c√≥ GPU
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**2
                cached = torch.cuda.memory_reserved() / 1024**2
                gpu_name = torch.cuda.get_device_name(0)
                print(f"[GPU] Device: {gpu_name}")
                print(f"[GPU] Memory: {allocated:.1f} MB allocated, {cached:.1f} MB cached")
                
                # Verify model ƒëang ·ªü GPU
                try:
                    if hasattr(model, 'model') and hasattr(model.model, 'device'):
                        model_device = str(model.model.device)
                        print(f"[GPU] ‚úÖ Model {model_id} ƒëang ·ªü: {model_device}")
                    else:
                        # Ki·ªÉm tra parameters c·ªßa model
                        try:
                            first_param = next(model.model.parameters()) if hasattr(model, 'model') else None
                            if first_param is not None:
                                param_device = str(first_param.device)
                                print(f"[GPU] ‚úÖ Model {model_id} parameters ƒëang ·ªü: {param_device}")
                        except:
                            pass
                except Exception as verify_error:
                    print(f"[WARN] Kh√¥ng th·ªÉ verify model device: {verify_error}")
            else:
                print(f"[CPU] Model {model_id} ch·∫°y tr√™n CPU (kh√¥ng c√≥ GPU)")
            
            print(f"[OK] Loaded model: {model_id} ({model_name}) for cameras: {cameras} on {device.upper()}")
            return True
            
        except Exception as e:
            print(f"[ERROR] Load model {model_id} failed: {e}")
            raise
    
    def _resolve_model_path(self, model_path: str) -> Optional[str]:
        """T√¨m ƒë∆∞·ªùng d·∫´n model (h·ªó tr·ª£ c·∫£ script v√† exe)"""
        # Th·ª≠ ƒë∆∞·ªùng d·∫´n tr·ª±c ti·∫øp
        if os.path.exists(model_path):
            return model_path
        
        # L·∫•y base directory
        if getattr(sys, 'frozen', False):
            # Ch·∫°y t·ª´ exe (PyInstaller)
            base_dir = os.path.dirname(sys.executable)
            meipass = getattr(sys, '_MEIPASS', base_dir)
            
            paths_to_try = [
                os.path.join(meipass, model_path),
                os.path.join(base_dir, model_path),
                os.path.join(os.getcwd(), model_path),
            ]
        else:
            # Ch·∫°y t·ª´ script
            base_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(base_dir))
            
            paths_to_try = [
                os.path.join(project_root, model_path),
                os.path.join(base_dir, model_path),
                os.path.join(os.getcwd(), model_path),
            ]
        
        for path in paths_to_try:
            if os.path.exists(path):
                return path
        
        return None
    
    def _extract_class_names(self, model) -> Dict[int, str]:
        """Tr√≠ch xu·∫•t class names t·ª´ model"""
        names = getattr(model, 'names', {})
        
        if isinstance(names, dict):
            return {int(k): v for k, v in names.items()}
        elif isinstance(names, list):
            return {i: name for i, name in enumerate(names)}
        
        return {}
    
    def _find_class_id(self, class_names: Dict[int, str], 
                       possible_names: List[str]) -> int:
        """T√¨m class ID theo t√™n"""
        for class_id, name in class_names.items():
            if name in possible_names:
                return class_id
        
        # M·∫∑c ƒë·ªãnh
        return 0 if 'person' in str(possible_names).lower() else 1
    
    def predict(self, camera_number: int, frame, 
                conf: float = 0.7, verbose: bool = False) -> Any:
        """Ch·∫°y inference tr√™n frame cho camera c·ª• th·ªÉ
        
        Args:
            camera_number: S·ªë th·ª© t·ª± camera (1, 2, 3, ...)
            frame: Frame video (numpy array)
            conf: Ng∆∞·ª°ng confidence
            verbose: In log hay kh√¥ng
            
        Returns:
            YOLO Results object
            
        Raises:
            RuntimeError: N·∫øu kh√¥ng t√¨m th·∫•y model cho camera
        """
        model_id = self._camera_model_map.get(camera_number)
        
        if not model_id:
            # Fallback: d√πng model ƒë·∫ßu ti√™n
            if self._models:
                model_id = list(self._models.keys())[0]
            else:
                raise RuntimeError(f"Kh√¥ng t√¨m th·∫•y model cho camera {camera_number}")
        
        model = self._models.get(model_id)
        if not model:
            raise RuntimeError(f"Model {model_id} ch∆∞a ƒë∆∞·ª£c load")
        
        with self._inference_locks[model_id]:
            # T·ªêI ∆ØU: S·ª≠ d·ª•ng GPU n·∫øu c√≥
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # Debug: Log device m·ªói 100 l·∫ßn ƒë·ªÉ verify
            if not hasattr(self, '_predict_count'):
                self._predict_count = {}
            if model_id not in self._predict_count:
                self._predict_count[model_id] = 0
            self._predict_count[model_id] += 1
            
            if self._predict_count[model_id] % 100 == 1 and verbose:
                print(f"[DEBUG] Model {model_id} inference #{self._predict_count[model_id]} on {device.upper()}")
            
            results = model.predict(
                frame, 
                conf=conf, 
                verbose=verbose, 
                task='segment',
                device=device  # Explicit device ƒë·ªÉ ƒë·∫£m b·∫£o d√πng GPU
            )
            return results[0] if results else None
    
    def track(self, camera_number: int, frame, 
              conf: float = 0.7, persist: bool = True, 
              verbose: bool = False) -> Any:
        """Ch·∫°y tracking tr√™n frame cho camera c·ª• th·ªÉ
        
        Args:
            camera_number: S·ªë th·ª© t·ª± camera (1, 2, 3, ...)
            frame: Frame video (numpy array)
            conf: Ng∆∞·ª°ng confidence
            persist: Gi·ªØ tracking ID qua c√°c frame
            verbose: In log hay kh√¥ng
            
        Returns:
            YOLO Results object v·ªõi tracking IDs
        """
        model_id = self._camera_model_map.get(camera_number)
        
        if not model_id:
            if self._models:
                model_id = list(self._models.keys())[0]
            else:
                raise RuntimeError(f"Kh√¥ng t√¨m th·∫•y model cho camera {camera_number}")
        
        model = self._models.get(model_id)
        if not model:
            raise RuntimeError(f"Model {model_id} ch∆∞a ƒë∆∞·ª£c load")
        
        with self._inference_locks[model_id]:
            # T·ªêI ∆ØU: S·ª≠ d·ª•ng GPU n·∫øu c√≥
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            results = model.track(
                frame,
                conf=conf,
                persist=persist,
                verbose=verbose,
                task='segment',
                device=device  # Explicit device ƒë·ªÉ ƒë·∫£m b·∫£o d√πng GPU
            )
            return results[0] if results else None
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin GPU status v√† verify models ƒëang ·ªü GPU
        
        Returns:
            Dict v·ªõi th√¥ng tin GPU:
            - gpu_available: bool
            - gpu_name: str
            - memory_allocated_mb: float
            - memory_reserved_mb: float
            - models_on_gpu: Dict[model_id, bool] - True n·∫øu model ƒëang ·ªü GPU
        """
        status = {
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": "N/A",
            "memory_allocated_mb": 0.0,
            "memory_reserved_mb": 0.0,
            "models_on_gpu": {}
        }
        
        if torch.cuda.is_available():
            status["gpu_name"] = torch.cuda.get_device_name(0)
            status["memory_allocated_mb"] = torch.cuda.memory_allocated() / 1024**2
            status["memory_reserved_mb"] = torch.cuda.memory_reserved() / 1024**2
            
            # Verify t·ª´ng model ƒëang ·ªü GPU
            for model_id, model in self._models.items():
                is_on_gpu = False
                try:
                    if hasattr(model, 'model') and hasattr(model.model, 'device'):
                        is_on_gpu = 'cuda' in str(model.model.device).lower()
                    elif hasattr(model, 'model'):
                        # Ki·ªÉm tra parameters
                        first_param = next(model.model.parameters(), None)
                        if first_param is not None:
                            is_on_gpu = 'cuda' in str(first_param.device).lower()
                except:
                    pass
                
                status["models_on_gpu"][model_id] = is_on_gpu
        
        return status
    
    def print_gpu_status(self) -> None:
        """In th√¥ng tin GPU status v√† verify models"""
        status = self.get_gpu_status()
        
        print("\n" + "="*60)
        print("üìä GPU STATUS & MODEL VERIFICATION")
        print("="*60)
        
        if status["gpu_available"]:
            print(f"‚úÖ GPU: {status['gpu_name']}")
            print(f"üì¶ Memory: {status['memory_allocated_mb']:.1f} MB / {status['memory_reserved_mb']:.1f} MB")
            print("\nüîç Model Location:")
            for model_id, is_on_gpu in status["models_on_gpu"].items():
                model_info = self._model_infos.get(model_id)
                model_name = model_info.name if model_info else model_id
                status_icon = "‚úÖ" if is_on_gpu else "‚ùå"
                device = "GPU" if is_on_gpu else "CPU"
                print(f"   {status_icon} {model_name} ({model_id}): {device}")
        else:
            print("‚ùå GPU kh√¥ng kh·∫£ d·ª•ng - T·∫•t c·∫£ models ch·∫°y tr√™n CPU")
            print("‚ö†Ô∏è  Warning: Performance s·∫Ω ch·∫≠m h∆°n ƒë√°ng k·ªÉ!")
        
        print("="*60 + "\n")
    
    def unload(self, model_id: str = None) -> None:
        """Gi·∫£i ph√≥ng model kh·ªèi memory
        
        Args:
            model_id: ID model c·∫ßn unload (None = t·∫•t c·∫£)
        """
        if model_id:
            if model_id in self._models:
                with self._inference_locks.get(model_id, threading.Lock()):
                    del self._models[model_id]
                    del self._model_infos[model_id]
                    # Remove camera mappings
                    self._camera_model_map = {
                        k: v for k, v in self._camera_model_map.items() 
                        if v != model_id
                    }
        else:
            # Unload t·∫•t c·∫£
            self._models.clear()
            self._model_infos.clear()
            self._camera_model_map.clear()


# Backward compatible alias
ModelLoader = MultiModelLoader
