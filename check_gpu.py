"""
Script kiá»ƒm tra GPU vÃ  verify models Ä‘ang cháº¡y trÃªn GPU

Usage:
    python check_gpu.py [model_path]
    
    VÃ­ dá»¥:
    python check_gpu.py best_segment_26_11.pt
"""

import sys
import os
import torch
from detection.model_loader import MultiModelLoader

def main():
    print("="*60)
    print("ğŸ” KIá»‚M TRA GPU VÃ€ MODEL STATUS")
    print("="*60)
    
    # 1. Kiá»ƒm tra GPU cÆ¡ báº£n
    print("\n1ï¸âƒ£ CUDA Availability:")
    if torch.cuda.is_available():
        print(f"   âœ… CUDA is available")
        print(f"   ğŸ“± Device: {torch.cuda.get_device_name(0)}")
        print(f"   ğŸ”¢ CUDA Version: {torch.version.cuda}")
        print(f"   ğŸ“¦ Number of GPUs: {torch.cuda.device_count()}")
    else:
        print("   âŒ CUDA is NOT available")
        print("   âš ï¸  Models sáº½ cháº¡y trÃªn CPU (cháº­m hÆ¡n 5-10x)")
        return
    
    # 2. GPU Memory Info
    print("\n2ï¸âƒ£ GPU Memory:")
    allocated = torch.cuda.memory_allocated() / 1024**2
    reserved = torch.cuda.memory_reserved() / 1024**2
    total = torch.cuda.get_device_properties(0).total_memory / 1024**2
    print(f"   ğŸ“Š Allocated: {allocated:.1f} MB")
    print(f"   ğŸ“¦ Reserved: {reserved:.1f} MB")
    print(f"   ğŸ’¾ Total: {total:.1f} MB")
    print(f"   ğŸ“ˆ Usage: {(allocated/total*100):.1f}%")
    
    # 3. Kiá»ƒm tra Model Loader hoáº·c load model tá»« command line
    print("\n3ï¸âƒ£ Model Loader Status:")
    loader = MultiModelLoader.get_instance()
    
    # Náº¿u cÃ³ model path tá»« command line, thá»­ load
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
        print(f"   ğŸ”„ Äang load model tá»«: {model_path}")
        try:
            # Kiá»ƒm tra file tá»“n táº¡i
            if not os.path.exists(model_path):
                print(f"   âŒ File khÃ´ng tá»“n táº¡i: {model_path}")
            else:
                memory_before = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
                success = loader.load(
                    model_id="test_model",
                    model_path=model_path,
                    model_name="Test Model",
                    cameras=[1]
                )
                if success:
                    memory_after = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
                    print(f"   âœ… Model loaded successfully!")
                    if torch.cuda.is_available() and memory_after > memory_before:
                        print(f"   ğŸ“Š GPU Memory tÄƒng: {memory_before:.1f} MB â†’ {memory_after:.1f} MB (+{memory_after-memory_before:.1f} MB)")
                else:
                    print(f"   âŒ KhÃ´ng thá»ƒ load model")
        except Exception as e:
            print(f"   âŒ Lá»—i khi load model: {e}")
    
    if not loader._models:
        print("   âš ï¸  ChÆ°a cÃ³ model nÃ o Ä‘Æ°á»£c load")
        print("   ğŸ’¡ CÃ¡ch 1: Cháº¡y á»©ng dá»¥ng chÃ­nh Ä‘á»ƒ load models")
        print("   ğŸ’¡ CÃ¡ch 2: python check_gpu.py <model_path>")
        print("   ğŸ’¡ VÃ­ dá»¥: python check_gpu.py best_segment_26_11.pt")
    else:
        print(f"   âœ… ÄÃ£ load {len(loader._models)} model(s)")
        loader.print_gpu_status()
    
    # 4. Test inference trÃªn GPU (náº¿u cÃ³ model)
    if loader._models:
        print("\n4ï¸âƒ£ Test Inference trÃªn GPU:")
        try:
            import numpy as np
            from ultralytics import YOLO
            
            # Láº¥y model Ä‘áº§u tiÃªn
            model_id = list(loader._models.keys())[0]
            model = loader._models[model_id]
            
            # Táº¡o dummy frame
            dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)
            
            # Test inference
            import time
            start = time.time()
            results = model.predict(
                dummy_frame,
                conf=0.5,
                verbose=False,
                task='segment',
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            elapsed = (time.time() - start) * 1000
            
            # Verify GPU memory tÄƒng sau inference
            after_allocated = torch.cuda.memory_allocated() / 1024**2
            
            print(f"   â±ï¸  Inference time: {elapsed:.1f}ms")
            print(f"   ğŸ“Š GPU Memory sau inference: {after_allocated:.1f} MB")
            
            if after_allocated > allocated:
                print(f"   âœ… GPU Ä‘Æ°á»£c sá»­ dá»¥ng (memory tÄƒng {after_allocated-allocated:.1f} MB)")
            else:
                print(f"   âš ï¸  GPU memory khÃ´ng tÄƒng - cÃ³ thá»ƒ Ä‘ang cháº¡y trÃªn CPU")
                
        except Exception as e:
            print(f"   âŒ Lá»—i test inference: {e}")
    
    print("\n" + "="*60)
    print("âœ… HoÃ n táº¥t kiá»ƒm tra")
    print("="*60)

if __name__ == "__main__":
    main()

